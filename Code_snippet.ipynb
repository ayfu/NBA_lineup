{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MySQLdb\n",
    "from sklearn.metrics import silhouette_score, mean_squared_error, precision_recall_curve\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class variable to connect with MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dbConnect():\n",
    "    '''\n",
    "    Class to help with context management for 'with' statements.\n",
    "    \n",
    "    Example:\n",
    "    c = dbConnect(host = 'localhost', user = 'root',\n",
    "                  passwd = 'default', db = 'nba_stats')\n",
    "    with c:\n",
    "        df.to_sql('nbadotcom', c.con, flavor = 'mysql', dtype = dtype)\n",
    "    '''\n",
    "    def __init__(self, host, user, passwd, db):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.passwd = passwd\n",
    "        self.db = db\n",
    "    def __enter__(self):\n",
    "        self.con = MySQLdb.connect(host = self.host, user = self.user,\n",
    "                                   passwd = self.passwd, db = self.db)\n",
    "        self.cur = self.con.cursor()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.cur.close()\n",
    "        self.con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example features that were explored: Team Statistics\n",
    "\n",
    "columns = ['W_opt', 'L_opt', 'W_PCT_opt', 'PCT_FGA_2PT_opt', 'PCT_FGA_3PT_opt',\n",
    "           'PCT_PTS_2PT_opt', 'PCT_PTS_2PT_MR_opt', 'PCT_PTS_3PT_opt',\n",
    "           'PCT_PTS_FB_opt', 'PCT_PTS_FT_opt', 'PCT_PTS_OFF_TOV_opt',\n",
    "           'PCT_PTS_PAINT_opt', 'PCT_AST_2PM_opt', 'PCT_UAST_2PM_opt',\n",
    "           'PCT_AST_3PM_opt', 'PCT_UAST_3PM_opt', 'PCT_AST_FGM_opt',\n",
    "           'PCT_UAST_FGM_opt', 'EFG_PCT_opt', 'FTA_RATE_opt', 'TM_TOV_PCT_opt',\n",
    "           'OREB_PCT_opt', 'OPP_EFG_PCT_opt', 'OPP_FTA_RATE_opt',\n",
    "           'OPP_TOV_PCT_opt', 'OPP_OREB_PCT_opt', 'PTS_OFF_TOV_opt',\n",
    "           'PTS_2ND_CHANCE_opt', 'PTS_FB_opt', 'PTS_PAINT_opt',\n",
    "           'OPP_PTS_OFF_TOV_opt', 'OPP_PTS_2ND_CHANCE_opt', 'OPP_PTS_FB_opt',\n",
    "           'OPP_PTS_PAINT_opt', 'FGM_opt', 'FGA_opt', 'FG_PCT_opt', 'FG3M_opt',\n",
    "           'FG3A_opt', 'FG3_PCT_opt', 'FTM_opt', 'FTA_opt', 'FT_PCT_opt',\n",
    "           'OREB_opt', 'DREB_opt', 'REB_opt', 'AST_opt', 'TOV_opt', 'STL_opt',\n",
    "           'BLK_opt', 'BLKA_opt', 'PF_opt', 'PFD_opt', 'PTS_opt',\n",
    "           'PLUS_MINUS_opt', 'OFF_RATING_opt', 'DEF_RATING_opt',\n",
    "           'NET_RATING_opt', 'AST_PCT_opt', 'AST_TO_opt', 'AST_RATIO_opt',\n",
    "           'DREB_PCT_opt', 'REB_PCT_opt', 'TS_PCT_opt', 'PACE_opt', 'PIE_opt',\n",
    "           'OPP_FGM_opt', 'OPP_FGA_opt', 'OPP_FG_PCT_opt', 'OPP_FG3M_opt',\n",
    "           'OPP_FG3A_opt', 'OPP_FG3_PCT_opt', 'OPP_FTM_opt', 'OPP_FTA_opt',\n",
    "           'OPP_FT_PCT_opt', 'OPP_OREB_opt', 'OPP_DREB_opt', 'OPP_REB_opt',\n",
    "           'OPP_AST_opt', 'OPP_TOV_opt', 'OPP_STL_opt', 'OPP_BLK_opt',\n",
    "           'OPP_BLKA_opt', 'OPP_PF_opt', 'OPP_PFD_opt', 'OPP_PTS_opt']\n",
    "\n",
    "# Parameters for kMeans clustering\n",
    "params_km = {'n_clusters': 5,\n",
    "             'max_iter': 10000,\n",
    "             'n_init': 10,\n",
    "             'init': 'k-means++',\n",
    "             'precompute_distances': 'auto',\n",
    "             'tol': 0.0001,\n",
    "             'n_jobs': 1,\n",
    "             'verbose': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class and function used for encoding categorical variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PruneLabelEncoder(LabelEncoder):\n",
    "    \"\"\"\n",
    "    Class variable that acts like LabelEncoder with the added functionality to \n",
    "    bin observations that appear with a low frequency defined by cutoff\n",
    "    \n",
    "    If the frequency of an encoded value is below the cutoff, it will bucket\n",
    "    everything to the first value it encounters that is below the cutoff value\n",
    "    \"\"\"\n",
    "    def __init___(self):\n",
    "        super(PruneLabelEncoder, self).__init__()\n",
    "    def fit(self, series, cutoff=10):\n",
    "        self.cutoff = cutoff\n",
    "        # Generate the transformation classes and the map for low output munging\n",
    "        super(PruneLabelEncoder, self).fit(series)\n",
    "        trans_series = super(PruneLabelEncoder, self).transform(series)\n",
    "        self.val_count_map = defaultdict(int)\n",
    "        for i in trans_series:\n",
    "            self.val_count_map[i] += 1\n",
    "        # Identify the first key with low frequency and use it for low freq vals\n",
    "        for key, val in self.val_count_map.items():\n",
    "            if val < self.cutoff:\n",
    "                self.low_cnt_target = key\n",
    "                break\n",
    "    def transform(self, series):\n",
    "        trans_series = super(PruneLabelEncoder, self).transform(series)\n",
    "        # Transform all the low frequency keys into the low frequency target key\n",
    "        for key, val in self.val_count_map.items():\n",
    "            if val < self.cutoff:\n",
    "                trans_series[trans_series==key] = self.low_cnt_target\n",
    "        return trans_series\n",
    "\n",
    "def encode(df, columns, TRANSFORM_CUTOFF):\n",
    "    '''\n",
    "    Takes in a dataframe, columns of interest, and a cutoff value for bucketing\n",
    "    encoding values\n",
    "\n",
    "    If the frequency of an encoded value is below the cutoff, it will bucket\n",
    "    everything to the first value it encounters that is below the cutoff value\n",
    "    '''\n",
    "    temp = df.copy()\n",
    "\n",
    "    # Checking if there are 2 or more unique values in each column\n",
    "    for x in columns:\n",
    "        if len(df[x].unique()) < 2:\n",
    "            return 'Error: Fewer than 2 unique values in a column'\n",
    "\n",
    "    for col in columns:\n",
    "        le = PruneLabelEncoder()\n",
    "        le.fit(df[col],TRANSFORM_CUTOFF)\n",
    "        df[col] = le.transform(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used for quickly evaluating kMeans  Clustering\n",
    "\n",
    "Used these functions to help decide which clusters to use for building my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def km_cluster(df, params_km, n = 3, rounds = 100,\n",
    "               columns = ['PLUS_MINUS_opt', 'PACE_opt']):\n",
    "    '''\n",
    "    Takes in dataframe (df), (n) number of clusters to explore,\n",
    "    (rounds) number of rounds, and columns\n",
    "\n",
    "    Returns a dataframe of the average and the standard deviation\n",
    "    of value_counts()\n",
    "    '''\n",
    "    params_km['n_clusters'] = n\n",
    "    dist = np.zeros(n)\n",
    "    p = pd.DataFrame({'pred_0': dist})\n",
    "    for x in range(rounds):\n",
    "        est = KMeans(**params_km)\n",
    "        X = df.as_matrix(columns).astype(float)\n",
    "        est.fit(X)\n",
    "        pred_km = est.predict(X)\n",
    "        p['pred_'+str(x)] = np.array(pd.Series(pred_km).value_counts())\n",
    "    results = pd.DataFrame({'mean': p.apply(np.mean, axis = 1),\n",
    "                            'std': p.apply(np.std, axis = 1)})\n",
    "    return results\n",
    "\n",
    "def km_silhouette(df, params_km,\n",
    "                  columns = ['PLUS_MINUS_opt', 'PACE_opt']):\n",
    "    '''\n",
    "    Takes in dataframe (df), (n) number of clusters to explore,\n",
    "    (rounds) number of rounds, and columns\n",
    "\n",
    "    Returns a dataframe of Silhouette Coefficients from kMeans\n",
    "    '''\n",
    "    \n",
    "    cluster = np.arange(2,30)\n",
    "    results = pd.DataFrame({'cluster': cluster})\n",
    "    score = []\n",
    "    for n in np.arange(2,30):\n",
    "        params_km['n_clusters'] = n\n",
    "        est = KMeans(**params_km)\n",
    "        X = df.as_matrix(columns).astype(float)\n",
    "        est.fit(X)\n",
    "        labels = est.labels_        \n",
    "        score += [silhouette_score(X, labels, metric='euclidean')]\n",
    "    results['score'] = score\n",
    "    return results\n",
    "\n",
    "def plot_silhouette(df, params_km, columns):\n",
    "    \"\"\"\n",
    "    Takes in dataframe (df), (n) number of clusters to explore,\n",
    "    (rounds) number of rounds, and columns\n",
    "    \n",
    "    Returns a plot of the Silhouette Coefficient vs. the\n",
    "    number of clusters from kMeans Clustering\n",
    "    \"\"\"\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(figsize = (10, 8))\n",
    "    \n",
    "    km_df = km_silhouette(df, params_km, columns)\n",
    "    \n",
    "    ax.plot(km_df['cluster'],km_df['score'], color = 'black',\n",
    "           label = 'Silhouette Coefficient',\n",
    "           lw = 3, alpha = 0.6, ls = 'dashed')\n",
    "    ax.set_xlabel('Number of kMeans Clusters',fontsize = 14)\n",
    "    ax.set_ylabel('Silhouette Coefficient', fontsize = 14)\n",
    "    ax.set_title('kMeans Analysis of Silhouette Coefficient', fontsize = 20)    \n",
    "\n",
    "def add_cluster(df, columns, params_km):\n",
    "    '''\n",
    "    Takes in: dataframe (df), columns to analyze, kMeans parameters (params_km)\n",
    "\n",
    "    Returns: Dataframe with a new column for the cluster (to help categorize)\n",
    "    '''\n",
    "    est = KMeans(**params_km)\n",
    "    X = df.as_matrix(columns).astype(float)\n",
    "    est.fit(X)\n",
    "    pred_km = est.predict(X)\n",
    "    df['cluster'] = pred_km\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new features to large dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_df(params_km = params_km, columns = columns):\n",
    "    '''\n",
    "    Takes in kMeans parameters and columns to perform kMeans\n",
    "\n",
    "    Returns a merged dataframe with a new 'cluster' column (from kMeans)\n",
    "    '''\n",
    "    \n",
    "    # Grab opponent stats from MySQL database\n",
    "    db = 'nba_stats'\n",
    "    temp = dbConnect(host = 'localhost', user = 'root',\n",
    "                     passwd = 'default', db = db)\n",
    "    with temp:\n",
    "        sql = 'SELECT * FROM nba_opponent_allstats_2015'\n",
    "        nba_df = pd.read_sql_query(sql, con = temp.con, index_col = 'index')\n",
    "\n",
    "    est = KMeans(**params_km)\n",
    "    X = nba_df.as_matrix(columns).astype(float)\n",
    "    est.fit(X)\n",
    "    pred_km = est.predict(X)\n",
    "    nba_df['cluster'] = pred_km\n",
    "    \n",
    "    # Merge full dataframe with new cluster feature\n",
    "    bigdf = pd.read_csv('../csv_data/nba_15season_all_150928.csv')\n",
    "    d = nba_df[['opponent','cluster']].copy()\n",
    "    bigdf = pd.merge(bigdf, d, how = 'left', on = 'opponent')\n",
    "    \n",
    "    # Add redundant column (using minutes_pm column to subset later)\n",
    "    # the 'minutes' column will be converted to avg minutes based on cluster\n",
    "    bigdf['minutes_pm'] = bigdf['minutes']\n",
    "    columns = list(bigdf.columns)\n",
    "    columns.remove('points')\n",
    "    columns.append('points')\n",
    "    bigdf = bigdf[columns]\n",
    "\n",
    "    return bigdf\n",
    "\n",
    "def cluster_features(bigdf):\n",
    "    '''\n",
    "    Takes in dataframe from make_df()\n",
    "\n",
    "    Returns a new dataframe that averages bref stats over each cluster for the\n",
    "    months not in March or April.\n",
    "    '''\n",
    "    # Columns for calculate summary stats over for each cluster\n",
    "    brefcol = ['minutes', 'num_poss', 'opp_poss', 'pace_bref',\n",
    "                'fg_pm', 'fga_pm', 'fg_percent', 'TP_pm',\n",
    "                'TPA_pm', 'TP_percent', 'eFG', 'FT_pm',\n",
    "                'FTA_pm', 'FT_percent', 'cluster']\n",
    "\n",
    "    df = bigdf.copy()\n",
    "    df_std = bigdf.copy()\n",
    "    df_max = bigdf.copy()\n",
    "    df_min = bigdf.copy()\n",
    "\n",
    "    for lineup in df['lineup'].unique():\n",
    "        # t_df is a dataframe of all information not associated with March or April\n",
    "        t_df = df[(df['lineup'] == lineup) &\n",
    "                  np.logical_not(df['month'].isin([3,4]))]\n",
    "        for x in t_df['cluster'].unique():\n",
    "            # Making mean columns (aggregate cluster by mean)\n",
    "            temp_df = pd.DataFrame(t_df.loc[t_df['cluster'] == x,\n",
    "                                   brefcol[:-1]].apply(np.mean, axis = 0))\n",
    "            temp_df = temp_df.T\n",
    "            temp_df = temp_df.as_matrix(brefcol[:-1])\n",
    "            df.loc[(df['lineup'] == lineup) & (df['cluster'] == x),\n",
    "                    brefcol[:-1]] = temp_df\n",
    "\n",
    "            # Making stdev columns\n",
    "            temp_std = pd.DataFrame(t_df.loc[t_df['cluster'] == x,\n",
    "                                   brefcol[:-1]].apply(np.std, axis = 0))\n",
    "            temp_std = temp_std.T\n",
    "            temp_std = temp_std.as_matrix(brefcol[:-1])\n",
    "            df_std.loc[(df_std['lineup'] == lineup) & (df_std['cluster'] == x),\n",
    "                    brefcol[:-1]] = temp_std\n",
    "\n",
    "            # Making max columns\n",
    "            temp_max = pd.DataFrame(t_df.loc[t_df['cluster'] == x,\n",
    "                                   brefcol[:-1]].apply(np.max, axis = 0))\n",
    "            temp_max = temp_max.T\n",
    "            temp_max = temp_max.as_matrix(brefcol[:-1])\n",
    "            df_max.loc[(df_max['lineup'] == lineup) & (df_max['cluster'] == x),\n",
    "                    brefcol[:-1]] = temp_max\n",
    "\n",
    "            # Making min columns\n",
    "            temp_min = pd.DataFrame(t_df.loc[t_df['cluster'] == x,\n",
    "                                   brefcol[:-1]].apply(np.min, axis = 0))\n",
    "            temp_min = temp_min.T\n",
    "            temp_min = temp_min.as_matrix(brefcol[:-1])\n",
    "            df_min.loc[(df_min['lineup'] == lineup) & (df_min['cluster'] == x),\n",
    "                    brefcol[:-1]] = temp_min\n",
    "    \n",
    "    # Renaming columns and dropping the cluster column (the last column)\n",
    "    df = df.drop('cluster', axis = 1)\n",
    "    \n",
    "    df_std = df_std[brefcol[:-1]]\n",
    "    df_std.columns = [col + '_std' for col in df_std.columns]\n",
    "\n",
    "    df_max = df_max[brefcol[:-1]]\n",
    "    df_max.columns = [col + '_max' for col in df_max.columns]\n",
    "\n",
    "    df_min = df_min[brefcol[:-1]]\n",
    "    df_min.columns = [col + '_min' for col in df_min.columns]\n",
    "\n",
    "    print 'df shape:', df.shape\n",
    "    print 'df_std shape:', df_std.shape\n",
    "    print 'df_max shape:', df_max.shape\n",
    "    print 'df_min shape:', df_min.shape\n",
    "\n",
    "    # Merge dataframes together on the index\n",
    "    df = df.reset_index()\n",
    "    df = pd.merge(df, df_std.reset_index(), how = 'left', on = 'index')\n",
    "    df = pd.merge(df, df_max.reset_index(), how = 'left', on = 'index')\n",
    "    df = pd.merge(df, df_min.reset_index(), how = 'left', on = 'index')\n",
    "    df = df.drop('index', axis = 1)\n",
    "    \n",
    "    # Placing the points column as the last column\n",
    "    columns = list(df.columns)   \n",
    "    columns.remove('points')\n",
    "    columns.append('points')\n",
    "    df = df[columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean  std\n",
      "0    12    0\n",
      "1    11    0\n",
      "2     7    0\n"
     ]
    }
   ],
   "source": [
    "# Example Usage          \n",
    "        \n",
    "# Grab opponent stats from MySQL database\n",
    "db = 'nba_stats'\n",
    "temp = dbConnect(host = 'localhost', user = 'root',\n",
    "                 passwd = 'default', db = db)\n",
    "with temp:\n",
    "    sql = 'SELECT * FROM nba_opponent_allstats_2015'\n",
    "    nba_df = pd.read_sql_query(sql, con = temp.con, index_col = 'index')\n",
    "\n",
    "df = encode(df = nba_df, columns = ['opponent'], TRANSFORM_CUTOFF = 1)\n",
    "# Columns from beginning of Notebook\n",
    "print km_cluster(df, params_km, n = 3, rounds = 100, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
